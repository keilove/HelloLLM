'''
涌现能力(Emergent Abilities)
是指在大型语言模型中，当模型的规模达到一定程度时，模型会展现出一些新的、复杂的能力，这些能力并不是通过简单的参数增加或训练数据扩展就能获得的。

上下文学习(In-context Learning)
是指模型在进行推理时，能够利用输入的上下文信息来辅助决策，而不需要依赖于外部知识库或额外的训练。这种能力使得模型能够更好地理解和处理复杂的任务。

指令微调(Instruction Tuning)
在模型训练完成后，进一步使用指令-响应对进行微调训练

逐步推理(Step-by-step Reasoning)
是指模型在处理复杂问题时，能够将问题分解为多个步骤进行推理，而不是一次性给出答案。这种能力使得模型能够更好地处理需要多步推理的任务。

幻觉(Hallucination)
是指模型在生成文本时，可能会产生一些与事实不符的信息或内容。这种现象通常是由于模型在训练过程中学习到了不准确的模式或信息所导致的。  

'''


'''
训练一个大模型的步骤
1. 预训练
Scaling Law: C ~ 6ND, 其中C为计算量， N为模型参数， D为训练的token数，训练token数应该是模型参数的1.7倍
分布式训练框架： 数据并行和模型并行。在数据并行和模型并行的思想基础上，还演化出了多种更高效的分布式方式，例如张量并行、3D 并行、ZeRO（Zero Redundancy Optimizer，
零冗余优化器）等。目前，主流的分布式训练框架包括 Deepspeed、Megatron-LM、ColossalAI 等，其中，Deepspeed 使用面最广。

2. 监督微调(SFT)
训练模型的“通用指令”遵循能力，一般通过指令微调来实现SFT。
构建多轮对话能力（完全依赖SFT阶段）

3. 强化学习微调(RLHF)
为chatgpt相较于GPT-3的最核心突破。这个过程中，SFT 是让 LLM 和人类的指令对齐，从而具有指令遵循能力；而 RLHF 则是从更深层次令 LLM 和人类价值观对齐，令其达到安全、有用、无害的核心标准。
RLHF分为两个步骤，训练RM(奖励模型)和PPO(近端策略优化算法)训练
'''